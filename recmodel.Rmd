---
title: "recmodel"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r setup, message=F, warning=F, echo=F}
#
require(tidyverse)
require(rstanarm)
require(magrittr)
require(loo)
require(bayesplot)
require(caret)
require(HSAUR3)
library(tidyverse)
library(caret)
#library(GGally)
library(ggplot2)
library(corrplot)
library(bayesplot)
theme_set(bayesplot::theme_default(base_family = "sans"))
library(rstanarm)
library(loo)
library(BFpack)
library(bayestestR)
library(logspline)
library(Stat2Data)
library(broom.mixed)

#library(projpred)
SEED=14124869

#
ggplot2::theme_set(ggplot2::theme_bw())
knitr::opts_chunk$set(echo = FALSE)
```


Spotify is a music streaming service with 356 million monthly active users and over 70 million tracks in their library. What drives their success? Aside from the size of the library, it is their ability to keep users engaged by providing a constant stream of relevant song and even podcast recommendations. In order to make these recommendations, there must be some data on songs as well as user preference data that can help decide which songs to recommend to a particular person. Some of Spotify's competitors such as Pandora will tag songs with attributes manually, while Spotify has the advantage of using deep learning models to do this as well as synthesize this with artist, genre, and user preference information in order to better inform recommendations. The three main recommendation models used at Spotify include collaborative filtering (which analyzes similarity in behavior between users), natural language processing or NLP (which extracts information from words in song lyrics, song titles, and playlist titles), and audio models (which detect patterns in raw audio content). (ADD CITATION)

Our data set came from Kaggle, and was based on one person's music preferences. Each observation in the data is a song on Spotify, and we are given information about each song, as well as whether or not the person liked the song. Continuous variables of interest that are between 0 and 1 include acousticness, danceability, energy, instrumentalness (amount of vocals in a song), liveness (how likely the song was recorded live), loudness, speechiness (amount of spoken words in a song), and valence (measure of how happy a song sounds). Other variables of interest include duration_ms (continuous variable denoting duration of the song in ms), key (categorical variable denoting the key of the song from 0-11), mode (modality of the song expressed as 0 or 1, corresponding to major or minor), tempo (continuous variable in BPM), time_signature (categorical variable from 1-5), and target (whether or not the person liked the song expressed as a 0 or 1). Most of these variables come from Spotify's automated deep learning models that processes the raw audio of songs. 

We wish to do a Bayesian logistic regression using the information about the song as predictors and target as the response variable. With such a model, the best use of it would be as a song recommender algorithm. We would be able to predict whether or not the person likes a song, and if we predict that they like the song, we would recommend it. One limitation of using this method to recommend a song is that it requires user input. Most users will not take the time to record whether or not they like or dislike songs that they come across, which is why Spotify's recommendation model takes into account factors such as time a song is played for or if a person has the song in a playlist in order to determine if a person likes a song or not. Despite our proposed method not being useful as a recommendation algorithm for Spotify itself, we do have the upside of being able to definitively know whether or not a person likes a song, rather than having to infer on it as Spotify does. So, for people who are fine with compiling a list of songs that they explicitly like or dislike, our model should be able to make a more targeted recommendation.



```{r read_data}
spotify <- read.csv("data.csv")
spotify <- as.data.frame(spotify)


smeans <- colMeans(spotify[2:14])
ssds <- apply(spotify[2:14], 2, sd)

spotify <- spotify %>% 
  mutate(key = factor(key)) %>% 
  mutate(mode = factor(mode)) %>% 
  mutate(time_signature = factor(time_signature)) %>% 
  mutate(target = factor(target)) %>%
  select(acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time_signature, valence, target)


spotify %>% 
  group_by(time_signature) %>% 
  count()

spotify %>% 
  group_by(target) %>% 
  count()


obs <- nrow(spotify)
rows <- sample(obs, replace = FALSE)
temp <- spotify[rows, ]
spotify_test <- temp %>% 
  slice(1:floor(.25*obs))
spotify_train <- temp %>%
  slice(floor(.25*obs)+1:obs)

unnorm_train <- spotify_train

```

The data set has 2017 songs. We decided to randomly divide this data, with 75% of it becoming the data on which we will train the model on, and the remaining 25% becoming data we will test the model on after it is created. We also check the distribution of target, since we want to make sure there are enough liked and disliked songs in the data set to be able to properly train our model. For example, we do not want to see only a handful of liked songs and almost all disliked songs, as this could result in a model that predicts almost every song as disliked, and will end up with a inflated correct prediction rate. However, as we can see, the data set is split almost into half liked songs and half disliked songs, which is beneficial for training our model.

For our purposes, since we are trying to model and predict whether or not this person likes a song, the response variable target takes the value of 0 or 1. That means our sampling model is a Bernoulli distribution with probability p. 

We were not able to find another logistic regression on a data set of songs on Spotify in order to predict whether or not someone likes a song. As a result, we did not have much information about what the coefficients might be, and decided to choose a weakly informative prior of Cauchy(0, 2.5) as recommended by (https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-4/A-weakly-informative-default-prior-distribution/10.1214/08-AOAS191.full).


```{r normalize}
c <- c(6,9,12,14)
for (i in 1:13) {
  if(!(is.element(i,c))){
      spotify_train[i] <- scale(spotify_train[i])
  }
}

sq <- 1:14
corrplot(cor(spotify_train[, sq[!sq %in% c]])) + title(main = "Correlation Plot", line = 3)



```



Because our continuous predictors are on different scales, we decided to normalize them. We then plot a correlation plot between each of the continuous predictors. As we can see, several variables have a large of correlation. Energy and acousticness as well as loudness and acousticness are negatively correlated, while energy and loudness are positively correlated. We are trying to avoid multicollinearity in our model, and since high correlation often is associated with multicollinearity, we will keep an eye on the pairs of variables with high magnitudes of correlation.



```{r}
# o_model <- stan_glm(
#   formula = target ~ acousticness+danceability+duration_ms+energy+instrumentalness+key+liveness+loudness+mode+speechiness+tempo+time_signature+valence,
#   data = spotify_train,
#   prior = cauchy(0, 2.5, autoscale=FALSE),
#   family = binomial(link = "logit")
# )
# # full_model$coefficients
# # acousticness+danceability+duration_ms+energy+instrumentalness+key+liveness+loudness+mode+speechiness+tempo+time_signature+valence
# 
# bayesfactor_parameters(o_model, null = 0)
# 
# BF(o_model)
# 
# 
# 
# half_model <- stan_glm(
#   formula = target ~ acousticness+danceability+duration_ms+instrumentalness+loudness+speechiness+tempo+valence,
#   data = spotify_train,
#   prior = cauchy(0, 2.5, autoscale=FALSE),
#   family = binomial(link = "logit")
# )
# 
# bayesfactor_parameters(half_model, null = 0)
# 
# BF(half_model)
# 
# 
# full_model <- stan_glm(
#   formula = target ~ acousticness+danceability+duration_ms+instrumentalness+loudness+speechiness+valence+mode*danceability+mode*valence+tempo*time_signature,
#   data = spotify_train,
#   prior = cauchy(0, 2.5, autoscale=FALSE), prior_intercept = cauchy(0, 2.5, autoscale=FALSE), QR=TRUE,
#   family = binomial(link = "logit")
# )
# 
# bayesfactor_parameters(full_model, null = 0)
# 
# BF(full_model)
# 
# 
# model <- stan_glm(
#   formula = target ~ acousticness+danceability+duration_ms+instrumentalness+loudness+speechiness+valence,
#   data = spotify_train,
#   prior = cauchy(0, 2.5, autoscale=FALSE), prior_intercept = cauchy(0, 2.5, autoscale=FALSE), QR=TRUE,
#   family = binomial(link = "logit")
# )
# 
# bayesfactor_parameters(model, null = 0)
# 
# BF(model)
```

In this project, we consider three different prediction models: a logistic regression model which uses Cauchy(0,2.5) priors for its coefficients, a logistic regression model which uses regularized horseshoe priors for its coefficients, and a "null" model which produces 1 (a "yes" for liking the song), regardless of input variables. We choose a Cauchy(0,2.5) prior for our first model because it is an empirically successful choice for weakly informative priors (https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-4/A-weakly-informative-default-prior-distribution/10.1214/08-AOAS191.full). Regularized horseshoe priors are typically meant for models in which the number of predictive variables is large in comparison to the number of samples. Although this is not the case in our experiment ($n=2017$, $p=13$), we were still interested to see whether this model could outperform our standard model.

We decided to backwards select our models using Bayes factors, which are useful in selecting Bayesian models. In our full model, we included all predictors with target as the response variable. We also decided to add some interaction effects. We thought that the mode variable (modality of a song, major or minor) would affect the danceability of a song as well as the valence (measurement of how happy a song sounds). We also thought that the time signature of a song might affect its tempo. As a result, we added interactions between mode and danceability, mode and valence, and time signature and tempo.

We calculated the Bayes factor of each coefficient compared to a point-null, which is the Savage-Dickey Ratio (https://easystats.github.io/bayestestR/articles/bayes_factors.html). We also checked the p value of a hypothesis test for each coefficient equaling 0. We dropped the term with the highest p value and lowest Bayes factor (these were always in agreement). We stopped when each term had a Bayes factor that can be interpreted as strong (or better) support for the coefficient not equaling zero. (https://www.statisticshowto.com/bayes-factor-definition/)



```{r model}
model <- stan_glm(
  formula = target ~ acousticness+danceability+duration_ms+instrumentalness+loudness+speechiness+valence,
  data = spotify_train,
  prior = cauchy(0, 2.5, autoscale=FALSE), prior_intercept = cauchy(0, 2.5, autoscale=FALSE), QR=TRUE,
  family = binomial(link = "logit"),
  refresh = 0
)

bayesfactor_parameters(model, null = 0)

BF(model)

round(coef(model), 3)
round(posterior_interval(model, prob = 0.95), 3)
mcmc_areas(as.matrix(model), prob = 0.95, prob_outer = 1) + geom_vline(xintercept=0)


summary(model)
tidy(model) %>% 
  knitr::kable(digits = 3)
```



```{r horseshoe}
n <- nrow(spotify_train)
p <- 13
p0 <- 7 # prior guess for the number of relevant variables
tau0 <- p0/(p-p0) * 1/sqrt(n)
hs_prior <- hs(df=1, global_df=1, global_scale=tau0)
c_prior <- cauchy(0, scale = 2.5, autoscale = FALSE)
hs_model <- stan_glm(formula = target ~ acousticness+danceability+instrumentalness+loudness+speechiness+valence,
                  data = spotify_train,
                  family = binomial(link = "logit"), 
                  prior = hs_prior, prior_intercept = c_prior,
                  seed = SEED, adapt_delta = 0.9,
                  refresh = 0)

bayesfactor_parameters(hs_model, null = 0)

BF(hs_model)


round(coef(hs_model), 3)
round(posterior_interval(hs_model, prob = 0.95), 3)
mcmc_areas(as.matrix(hs_model), prob = 0.95, prob_outer = 1) + geom_vline(xintercept=0)





summary(hs_model)
tidy(hs_model) %>% 
  knitr::kable(digits = 3)
```


```{r null}
model1 <- update(model, formula = target ~ 1, QR = FALSE)

bayesfactor_parameters(model1, null = 0)

BF(model1)


round(coef(model1), 3)
round(posterior_interval(model1, prob = 0.95), 3)
mcmc_areas(as.matrix(model1), prob = 0.95, prob_outer = 1) + geom_vline(xintercept=0)


```
```{r conditions}

par(mfrow=c(3,3))
emplogitplot1(target ~ instrumentalness, data = spotify_train, 
              ngroups = 5)
emplogitplot1(target ~ acousticness, data = spotify_train, 
              ngroups = 10)
emplogitplot1(target ~ danceability, data = spotify_train, 
              ngroups = 10)
emplogitplot1(target ~ duration_ms, data = spotify_train, 
              ngroups = 10)
emplogitplot1(target ~ loudness, data = spotify_train, 
              ngroups = 10)
emplogitplot1(target ~ speechiness, data = spotify_train, 
              ngroups = 10)
emplogitplot1(target ~ valence, data = spotify_train, 
              ngroups = 10)

car::vif(model)

car::vif(hs_model)

```

Now that our models are created, we check our model conditions for logistic regression. The first condition is linearity. As is shown by the graphs above, each of the continuous predictors in our models appear to have a more or less linear relationship with the log odds of the person liking a song. As a result, the linearity condition is satisfied. 

The next condition is the randomness condition. Because the samples are not stated to be truly randomly taken we must consider whether or not the observations differ systemically from our population of interest. The population of interest is songs that the person who made the data set listens to and rates as a like or a dislike, and so there is no reason to believe that the sample is not representative of the population. The randomness condition is satisfied.

The next condition is the independence condition. We have no reason to believe that each song is not independent from one another, and so the independence condition is satisfied. 

Lastly, we check multicollinearity by checking the variance inflation factors of each of the variables. As seen in the output for both cauchy and horseshoe, since all VIF values are under 10, multicollinearity is not a problem in our model.



```{r comparison}
(loo1 <- loo(model, save_psis = TRUE))

(loo1 <- loo(model1, save_psis = TRUE))

(loo1 <- loo(hs_model, save_psis = TRUE))
```





```{r prediction}
linpred <- posterior_linpred(model)
preds <- posterior_epred(model)
pred <- colMeans(preds)
pr <- as.integer(pred >= 0.5)
   
# posterior classification accuracy
round(mean(xor(pr,as.integer(spotify_train$target==0))),3)
```

To compare the predictive performance of our three models, we use leave-one-out cross-validation (LOO). In particular, we use the LOO Information Criterion (LOOIC) as a metric by which to measure the predictive performance of our models. LOOIC measures the predictive accuracy of a given model by leaving a single sample out of the training set and training the model on the all other samples in the dataset. It then tests its accuracy on this single left-out sample and repeats this process for all other samples in the training set. LOOIC reports a metric based on the accuracy of these LOO predictions, where a lower LOOIC score corresponds to a better model (http://mc-stan.org/rstanarm/reference/loo.stanreg.html). The LOOIC values for all three models are shown below. Of the three models, we see that *** performed the best. 

With our chosen model, we now want to be able to use it on song data we may find in the wild. However, our current model was fit on a normalized dataset, so its parameters are only trained to classify songs by characteristics with mean 0 and standard deviation 1. To retrieve the parameters for a model that works on unnormalized data ($x_i$) instead of normalized data $\tilde{x}_i$, we adjust our coefficients as follows:

$$
\begin{equation*}
\begin{split}
  \theta &= \beta_0 + \beta_1 \tilde{x}_1 + \ldots + \beta_p \tilde{x}_p \\
  &= \beta_0 + \beta_1\Big(\frac{x_1-\overline{x}_1}{\sigma_1}\Big) + \ldots + \beta_p\Big(\frac{x_p-\overline{x}_p}{\sigma_p}\Big) \\
  &= \beta_0 + \Big(\frac{\beta_1}{\sigma_1}x_1 - \frac{\beta_1\overline{x}_1}{\sigma_1}\Big) + \ldots + \Big(\frac{\beta_p}{\sigma_p}x_p - \frac{\beta_p\overline{x}_p}{\sigma_p}\Big) \\
  &= \Big( \beta_0 - \frac{\beta_1\overline{x}_1}{\sigma_1} - \ldots - \frac{\beta_p\overline{x}_p}{\sigma_p} \Big) + \frac{\beta_1}{\sigma_1}x_1 + \ldots + \frac{\beta_p}{\sigma_p}x_p \\
  &:= \beta_0^* + \beta_1^*x_1 + \ldots + \beta_p^*x_p
\end{split}
\end{equation*}
$$

```{r unormalize}
vars <- c("acousticness", "danceability", "duration_ms", "instrumentalness", "loudness", "speechiness", "valence")
coefficients <- coef(model)
for(var in vars){
  coefficients[var] <- coefficients[var]/ssds[var]
  coefficients["(Intercept)"] <- coefficients["(Intercept)"] - coefficients[var]*smeans[var]
}

```


```{r}
spotify_train2 <- unnorm_train %>% 
  select(acousticness, danceability, duration_ms, instrumentalness, loudness, speechiness, valence)
ip <- as.matrix(spotify_train2) %*% (as.matrix(coefficients)[1:ncol(spotify_train2)+1]) + rep(as.matrix(coefficients)[1],nrow(spotify_train2))
yhat <- exp(ip)/(1+exp(ip)) >= 0.5


yhat <- as.integer(yhat)
ytrain <- spotify_train$target
mean(yhat == ytrain)

spotify_test2 <- spotify_test %>% 
  select(acousticness, danceability, duration_ms, instrumentalness, loudness, speechiness, valence)
ip <- as.matrix(spotify_test2) %*% (as.matrix(coefficients)[1:ncol(spotify_test2)+1]) + rep(as.matrix(coefficients)[1],nrow(spotify_test2))
yhat_test <- exp(ip)/(1+exp(ip)) >= 0.5


yhat_test <- as.integer(yhat_test)
ytest <- spotify_test$target
mean(yhat_test == ytest) 
```